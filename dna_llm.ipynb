{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPduRy1QCTdimOlBEqm6SLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valasik0/dna-sequence-llm/blob/first-prototype-test/dna_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IispHeKQBkgf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import gzip\n",
        "import wandb\n",
        "import yaml\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLtDT8SLB00_",
        "outputId": "13824046-e283-42b5-842b-c19581a82701"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    vocab_size: int = 5\n",
        "    max_len: int = 256\n",
        "    d_model: int = 128\n",
        "    n_heads: int = 4\n",
        "    n_layers: int = 4\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    mask_prob: float = 0.15\n",
        "    mode: str = \"fasta\"\n",
        "    n_samples: int = 1000\n",
        "    max_len_fasta: int = 15000\n",
        "    experiment_name: str = \"dna_transformer_baseline\"\n",
        "    run_name: str = None\n",
        "\n",
        "def setup_experiment(config: ExperimentConfig):\n",
        "    if config.run_name is None:\n",
        "        config.run_name = f\"{config.experiment_name}_{int(time.time())}\"\n",
        "    output_dir = f\"outputs/{config.run_name}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    with open(f\"{output_dir}/config.txt\", \"w\") as f:\n",
        "        f.write(str(asdict(config)))\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "dRVNJSz55rzC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DNATokenizer:\n",
        "    def __init__(self, method=\"single\", k=3):\n",
        "        self.method = method\n",
        "        self.k = k\n",
        "\n",
        "        if method == \"single\":\n",
        "            self.vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'MASK': 4}\n",
        "        elif method == \"kmer\":\n",
        "            self.vocab = self._build_kmer_vocab(k)\n",
        "        else:\n",
        "          pass\n",
        "          #dalsi tokenizery (pozdejsi experimenty)\n",
        "\n",
        "    def _build_kmer_vocab(self, k):\n",
        "        bases = ['A', 'C', 'G', 'T']\n",
        "        kmers = [''.join(p) for p in itertools.product(bases, repeat=k)]\n",
        "        vocab = {kmer: i for i, kmer in enumerate(kmers)}\n",
        "        vocab['MASK'] = len(vocab)\n",
        "        return vocab\n",
        "\n",
        "    def encode(self, sequence):\n",
        "        if self.method == \"single\":\n",
        "            return [self.vocab.get(x, 0) for x in sequence.upper()]\n",
        "        elif self.method == \"kmer\":\n",
        "            tokens = []\n",
        "            for i in range(len(sequence) - self.k + 1):\n",
        "                kmer = sequence[i:i+self.k]\n",
        "                tokens.append(self.vocab.get(kmer, 0))\n",
        "            return tokens"
      ],
      "metadata": {
        "id": "xRrpQAPX5xmP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def span_mask(x, mask_prob=0.15, span_len_range=(3, 8), mask_idx=4):\n",
        "    \"\"\"Span masking pro simulaci strukturálních variant\"\"\"\n",
        "    masked = x.clone()\n",
        "    labels = x.clone()\n",
        "    labels.fill_(-100)  # ignore index\n",
        "\n",
        "    for i in range(x.size(0)):\n",
        "        pos = 0\n",
        "        while pos < x.size(1):\n",
        "            if random.random() < mask_prob:\n",
        "                span_len = random.randint(*span_len_range)\n",
        "                end_pos = min(pos + span_len, x.size(1))\n",
        "                masked[i, pos:end_pos] = mask_idx\n",
        "                labels[i, pos:end_pos] = x[i, pos:end_pos]\n",
        "                pos = end_pos\n",
        "            else:\n",
        "                pos += 1\n",
        "\n",
        "    return masked, labels"
      ],
      "metadata": {
        "id": "DWriUKHG6bnH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_input(x, mask_prob=0.15):\n",
        "    masked = x.clone()\n",
        "    mask = torch.rand_like(x.float()) < mask_prob\n",
        "    masked[mask] = 4\n",
        "    labels = x.clone()\n",
        "    labels[~mask] = -100\n",
        "    return masked, labels"
      ],
      "metadata": {
        "id": "uFF541uI8RFi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def curriculum_mask(x, epoch, total_epochs, base_prob=0.15, max_prob=0.30, mask_idx=4):\n",
        "    \"\"\"Postupné zvyšování obtížnosti maskování\"\"\"\n",
        "    progress = epoch / total_epochs\n",
        "    current_prob = base_prob + (max_prob - base_prob) * progress\n",
        "    return mask_input(x, mask_prob=current_prob, mask_idx=mask_idx)"
      ],
      "metadata": {
        "id": "OZ3lLKF17lhi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, sequences, tokenizer, device, mask_strategy=\"random\"):\n",
        "    \"\"\"Základní vyhodnocení modelu\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(sequences), 64):  # batch_size = 64\n",
        "            batch_seqs = sequences[i:i+64]\n",
        "            batch_tokens = [tokenizer.encode(seq) for seq in batch_seqs]\n",
        "            batch_tokens = [t for t in batch_tokens if len(t) == model.max_len]\n",
        "\n",
        "            if len(batch_tokens) == 0:\n",
        "                continue\n",
        "\n",
        "            x = torch.tensor(batch_tokens, dtype=torch.long).to(device)\n",
        "\n",
        "            if mask_strategy == \"random\":\n",
        "                masked_x, labels = mask_input(x)\n",
        "            else:\n",
        "                masked_x, labels = span_mask(x)\n",
        "\n",
        "            logits = model(masked_x)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, model.vocab_size),\n",
        "                labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "\n",
        "            # Accuracy pouze na maskovaných pozicích\n",
        "            mask_positions = (labels != -100)\n",
        "            if mask_positions.sum() > 0:\n",
        "                pred = logits.argmax(dim=-1)\n",
        "                acc = (pred == labels)[mask_positions].float().mean()\n",
        "                total_acc += acc.item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return {\n",
        "        'eval_loss': total_loss / num_batches if num_batches > 0 else float('inf'),\n",
        "        'eval_accuracy': total_acc / num_batches if num_batches > 0 else 0.0\n",
        "    }"
      ],
      "metadata": {
        "id": "nuNXs5GJ6hNb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_tracking(config: ExperimentConfig):\n",
        "    output_dir = setup_experiment(config)\n",
        "\n",
        "    tokenizer = DNATokenizer(method=\"single\")\n",
        "\n",
        "    sequences = get_sequences(\n",
        "        mode=config.mode,\n",
        "        n=config.n_samples,\n",
        "        L=config.max_len,\n",
        "        fasta_path=fasta_path if config.mode == 'fasta' else None,\n",
        "        max_len_fasta=config.max_len_fasta\n",
        "    )\n",
        "\n",
        "    split_idx = int(0.8 * len(sequences))\n",
        "    train_sequences = sequences[:split_idx]\n",
        "    val_sequences = sequences[split_idx:]\n",
        "\n",
        "    model = SimpleDNATransformer(\n",
        "        vocab_size=config.vocab_size,\n",
        "        max_len=config.max_len,\n",
        "        d_model=config.d_model,\n",
        "        n_heads=config.n_heads,\n",
        "        n_layers=config.n_layers\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=0.01)\n",
        "    train_batches = prepare_batches(train_sequences, config.max_len)\n",
        "    print(f\"Tréninková data: {len(train_batches)} batchů\")\n",
        "\n",
        "    train_losses, eval_losses, eval_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        indices = random.sample(range(len(train_batches)), min(config.batch_size, len(train_batches)))\n",
        "        x = torch.tensor([train_batches[i] for i in indices], dtype=torch.long).to(DEVICE)\n",
        "        masked_x, labels = mask_input(x, mask_prob=config.mask_prob)\n",
        "        logits = model(masked_x)\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, config.vocab_size),\n",
        "            labels.view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss = loss.item()\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Vyhodnocení každých 5 epoch\n",
        "        if epoch % 5 == 0:\n",
        "            eval_metrics = evaluate_model(model, val_sequences, tokenizer, DEVICE)\n",
        "            eval_losses.append(eval_metrics[\"eval_loss\"])\n",
        "            eval_accuracies.append(eval_metrics[\"eval_accuracy\"])\n",
        "            print(f\"Epoch {epoch}: train_loss={epoch_loss:.4f}, eval_loss={eval_metrics['eval_loss']:.4f}, eval_acc={eval_metrics['eval_accuracy']:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch}: train_loss={epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"Ukládám model a statistiky…\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'tokenizer_vocab': tokenizer.vocab\n",
        "    }, f\"{output_dir}/model_final.pt\")\n",
        "\n",
        "    # Výsledky tréninku (možno i vizualizovat v Colabu)\n",
        "    with open(f\"{output_dir}/train_losses.txt\", \"w\") as f:\n",
        "        for l in train_losses:\n",
        "            f.write(f\"{l}\\n\")\n",
        "    with open(f\"{output_dir}/eval_losses.txt\", \"w\") as f:\n",
        "        for l in eval_losses:\n",
        "            f.write(f\"{l}\\n\")\n",
        "    with open(f\"{output_dir}/eval_accuracies.txt\", \"w\") as f:\n",
        "        for l in eval_accuracies:\n",
        "            f.write(f\"{l}\\n\")\n",
        "\n",
        "    print(f\"Experiment: {config.run_name} dokončen. Výsledky v {output_dir}.\")\n",
        "    return model, output_dir"
      ],
      "metadata": {
        "id": "uVRbWwci3Fn4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDNATransformer(nn.Module):\n",
        "    def __init__(self, vocab_size=4, max_len=256, d_model=128, n_heads=4, n_layers=4):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        x = self.embed(x) + self.pos_embed[:, :seq_len]\n",
        "        x = self.encoder(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Pva7FfhY-KII"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "    ExperimentConfig(experiment_name=\"baseline_single_token\"),\n",
        "    ExperimentConfig(d_model=256, experiment_name=\"larger_model\"),\n",
        "    ExperimentConfig(mask_prob=0.25, experiment_name=\"higher_masking\"),\n",
        "    ExperimentConfig(n_samples=2000, experiment_name=\"more_data\")\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\n=== Spouštím experiment: {config.experiment_name} ===\")\n",
        "    model, output_dir = train_with_tracking(config)\n",
        "    print(f\"Výsledky uloženy v: {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeTV9kUj7Oy3",
        "outputId": "caa7d1ce-ab7d-41a8-ed90-491fc2ec75ce"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Spouštím experiment: baseline_single_token ===\n",
            "Tréninková data: 46 batchů\n",
            "Epoch 0: train_loss=1.5323, eval_loss=1.7982, eval_acc=0.2369\n",
            "Epoch 1: train_loss=1.8369\n",
            "Epoch 2: train_loss=1.4034\n",
            "Epoch 3: train_loss=1.6598\n",
            "Epoch 4: train_loss=1.5628\n",
            "Epoch 5: train_loss=1.4241, eval_loss=1.3988, eval_acc=0.2727\n",
            "Epoch 6: train_loss=1.3944\n",
            "Epoch 7: train_loss=1.4174\n",
            "Epoch 8: train_loss=1.4012\n",
            "Epoch 9: train_loss=1.3695\n",
            "Ukládám model a statistiky…\n",
            "Experiment: baseline_single_token_1753269044 dokončen. Výsledky v outputs/baseline_single_token_1753269044.\n",
            "Výsledky uloženy v: outputs/baseline_single_token_1753269044\n",
            "\n",
            "=== Spouštím experiment: larger_model ===\n",
            "Tréninková data: 46 batchů\n",
            "Epoch 0: train_loss=1.6131, eval_loss=1.9518, eval_acc=0.2774\n",
            "Epoch 1: train_loss=1.9011\n",
            "Epoch 2: train_loss=1.9229\n",
            "Epoch 3: train_loss=1.6985\n",
            "Epoch 4: train_loss=1.3899\n",
            "Epoch 5: train_loss=1.5987, eval_loss=1.7249, eval_acc=0.2857\n",
            "Epoch 6: train_loss=1.6530\n",
            "Epoch 7: train_loss=1.5712\n",
            "Epoch 8: train_loss=1.4550\n",
            "Epoch 9: train_loss=1.3987\n",
            "Ukládám model a statistiky…\n",
            "Experiment: larger_model_1753269116 dokončen. Výsledky v outputs/larger_model_1753269116.\n",
            "Výsledky uloženy v: outputs/larger_model_1753269116\n",
            "\n",
            "=== Spouštím experiment: higher_masking ===\n",
            "Tréninková data: 46 batchů\n",
            "Epoch 0: train_loss=2.0097, eval_loss=1.5215, eval_acc=0.2613\n",
            "Epoch 1: train_loss=1.5022\n",
            "Epoch 2: train_loss=1.7411\n",
            "Epoch 3: train_loss=1.6347\n",
            "Epoch 4: train_loss=1.4724\n",
            "Epoch 5: train_loss=1.4014, eval_loss=1.4401, eval_acc=0.2168\n",
            "Epoch 6: train_loss=1.4170\n",
            "Epoch 7: train_loss=1.4270\n",
            "Epoch 8: train_loss=1.4107\n",
            "Epoch 9: train_loss=1.3885\n",
            "Ukládám model a statistiky…\n",
            "Experiment: higher_masking_1753269220 dokončen. Výsledky v outputs/higher_masking_1753269220.\n",
            "Výsledky uloženy v: outputs/higher_masking_1753269220\n",
            "\n",
            "=== Spouštím experiment: more_data ===\n",
            "Tréninková data: 46 batchů\n",
            "Epoch 0: train_loss=1.8098, eval_loss=1.8387, eval_acc=0.2787\n",
            "Epoch 1: train_loss=1.8038\n",
            "Epoch 2: train_loss=1.6172\n",
            "Epoch 3: train_loss=1.4439\n",
            "Epoch 4: train_loss=1.4209\n",
            "Epoch 5: train_loss=1.4006, eval_loss=1.4063, eval_acc=0.2409\n",
            "Epoch 6: train_loss=1.4072\n",
            "Epoch 7: train_loss=1.4024\n",
            "Epoch 8: train_loss=1.3860\n",
            "Epoch 9: train_loss=1.3838\n",
            "Ukládám model a statistiky…\n",
            "Experiment: more_data_1753269283 dokončen. Výsledky v outputs/more_data_1753269283.\n",
            "Výsledky uloženy v: outputs/more_data_1753269283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_path = \"/content/drive/MyDrive/SP/GCF_000001405.26_GRCh38_genomic.fna.gz\"\n",
        "L = 256           #velikost okna (context window)\n",
        "VOCAB_SIZE = 5    #A, C,G,T,MASK\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "MASK_IDX = 4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Wu6etD-SJczW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_random_seq(n=10000, L=256, alphabet='ACGT'):\n",
        "    for _ in range(n):\n",
        "        yield ''.join(random.choice(alphabet) for _ in range(L))"
      ],
      "metadata": {
        "id": "iNOtYhvs8uNj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequences(mode='random', n=10000, L=256, fasta_path=None, max_len_fasta=None):\n",
        "    if mode == 'random':\n",
        "        return list(gen_random_seq(n, L))\n",
        "    elif mode == 'fasta':\n",
        "        assert fasta_path is not None, 'File path not found'\n",
        "        full_seq = read_fasta(fasta_path, max_length=max_len_fasta if max_len_fasta else L*n)\n",
        "        return [full_seq[i:i+L] for i in range(0, len(full_seq) - L + 1, L)]\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'random' or 'fasta'\")"
      ],
      "metadata": {
        "id": "8auFdafb8pwD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_to_tokens(seq):\n",
        "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "    seq = seq.upper()\n",
        "    return [mapping.get(x, 0) for x in seq if x in mapping]\n",
        ""
      ],
      "metadata": {
        "id": "RWr5ysYY87sn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batches(sequences, L=256):\n",
        "    token_batches = []\n",
        "    for seq in sequences:\n",
        "        tokens = seq_to_tokens(seq)\n",
        "        if len(tokens) == L:\n",
        "            token_batches.append(tokens)\n",
        "    return token_batches"
      ],
      "metadata": {
        "id": "XDvz6dre80YQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_fasta(filepath, max_length=10000):\n",
        "    seq = []\n",
        "    total_len = 0\n",
        "    valid_bases = set('ACTG')\n",
        "\n",
        "    with gzip.open(filepath, 'rt') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                continue\n",
        "            line = line.upper()\n",
        "            filtered = ''.join([c for c in line if c in valid_bases])  #jen ACTG\n",
        "            to_take = max_length - total_len\n",
        "            if to_take <= 0:\n",
        "                break\n",
        "            seq.append(filtered[:to_take])\n",
        "            total_len += len(filtered[:to_take])\n",
        "            if total_len >= max_length:\n",
        "                break\n",
        "\n",
        "    return ''.join(seq)"
      ],
      "metadata": {
        "id": "sNWLuGunDKNy"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}